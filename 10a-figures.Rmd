---
title: "Facts and figures"
output: html_notebook
---

# Libraries

```{r}
library(tidyverse)
library(lubridate)
library(stringr)
library(e1071)
library(ReporteRs)
library(slam)
library(tm)
library(topicmodels)
library(stm)
library(foreach)
```

# Constants

```{r}
theme_sparse <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
  panel.background = element_blank(), axis.line = element_line(colour = "black"), legend.key = element_blank())
```

# Data exploration

```{r}
# Load posts
posts <- unz("data/PostsData.zip", filename = "posts.csv") %>%
  read.csv(header = TRUE, sep = ",", stringsAsFactors = FALSE)

# Fix data fields
posts <- posts %>% mutate(
  CreationDate = ymd_hms(CreationDate),
  Name = factor(Name)
)
```

```{r}
# Post stats
summary(posts$Name)
min(posts$CreationDate)
max(posts$CreationDate)
```

```{r}
# Load data file
load(file = "data/sparse_matrix_no_tex.zip")
```

```{r}
# Data file stats
sparse$dimnames$Docs %>% length()
sparse$dimnames$Terms %>% length()
print("Min word length")
col_sums(sparse) %>% min()
```

```{r, cache = TRUE}
# Word count comparison
cb_dtm <- DocumentTermMatrix(Corpus(VectorSource(posts$CleanBody)))
```
```{r}
# Full document word count
cb_dtm$dimnames$Terms %>% length()
col_sums(cb_dtm) %>% sum()
# Trimmed document word count
sparse$dimnames$Terms %>% length()
col_sums(sparse) %>% sum()
```

## FIG. Word-document distribution

```{r, fig.width = 5, fig.height = 2.5}
# Data
wc <- row_sums(sparse) %>% as.numeric()
# Stats
log10(wc) %>% mean()
log10(wc) %>% sd()
log10(wc) %>% skewness()
log10(wc) %>% kurtosis(type = 2) + 3

# Plot
myplot <- wc %>% data.frame(wordcount = .) %>%
  ggplot(aes(x = wordcount)) +
  geom_histogram(binwidth = .06, color = "lightgray", fill = "lightgray") +
  stat_function(
    fun = function(x) {(dnorm(log10(x), mean(log10(wc)), sd(log10(wc)))) * length(wc) * .06},
    color = "blue", size = 1, alpha = .3
  ) +
  scale_x_log10(breaks = c(0, 1, 10, 100, 1000, 10000)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Word count", y = "Document count") +
  theme_sparse

plot(myplot)
```
```{r}
# Write output
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(myplot), width = 5, height = 3, vector.graphic = TRUE)
writeDoc(mydoc, file = "results/fig/1-word-doc-dist.docx")
```

# LDA

```{r}
# Load precomputed model perplexities
lda_results <- read.csv("results/lda_k_perp.csv")
```

```{r, rows.print = 15}
# Average convergence
lda_results %>% group_by(model) %>% summarise(mean(time))
```

## FIG. LDA runtime and perplexity

```{r}
# LDA runtime
lda1 <- lda_results %>% filter(model >= 10 & model %% 5 == 0) %>% group_by(model) %>% summarise(time = mean(time)) %>%
  ggplot(aes(x = model, y = time)) + geom_smooth(method = "lm", alpha = .5, fullrange = TRUE) +
  geom_point() + xlim(c(9, 51)) + scale_y_continuous(breaks = seq(0, 12, by = 2), expand = c(.005, 0)) +
  labs(x = expression(paste(italic("K"))), y = "Time elapsed (hours)") + theme_sparse
plot(lda1)

# LDA perplexity
lda2 <- lda_results %>% filter(model >= 10) %>% group_by(model) %>% mutate(in_avg = mean(in_sample), out_avg = mean(out_sample)) %>%
  ggplot(aes(x = model)) +
  geom_point(aes(y = in_sample, color = "In-sample"), alpha = .5) + geom_line(aes(y = in_avg, color = "In-sample"), size = 1, alpha = .5) +
  geom_point(aes(y = out_sample, color = "Out-sample"), alpha = .5) + geom_line(aes(y = out_avg, color = "Out-sample"), size = 1, alpha = .5) +
  labs(x = expression(paste(italic("K"))), y = "Perplexity", color = "") + theme_sparse 
plot(lda2)  
```

```{r}
# Write plots
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(lda1), width = 5, height = 3, vector.graphic = TRUE)
mydoc <- addPlot(mydoc, function() print(lda2), width = 5, height = 3, vector.graphic = TRUE)
writeDoc(mydoc, file = "results/fig/2-LDA.docx")
```

# Best LDA model

```{r}
# Load model
load("results/lda_final_40.zip")
```

```{r}
# How frequently are the topics expected to show up?
lda_z_counts <- models40f@gamma %>% colSums() %>% data.frame(topic = 1:40, count = .) %>% mutate(pct = count/sum(count))
(lda_z_counts <- lda_z_counts %>% arrange(desc(pct)) %>% mutate(cumpct = cumsum(pct)))
```

```{r}
# How frequently do each of the topics actually show up?
models40f@wordassignments$v %>% factor() %>% summary() %>% data.frame(topic = 1:40, count = .) %>%
    mutate(pct = count / sum(count)) %>% arrange(desc(count)) %>% mutate(cumpct = cumsum(pct))
```

```{r}
# Most likely documents
lda_likely <- which(models40f@loglikelihood >= tail(sort(models40f@loglikelihood))[1])
{posts %>% select(Id, Title, CleanBody)}[lda_likely, ]
```

## FIG. topic frequency

```{r, rows.print = 15}
lda3 <- lda_z_counts %>% mutate(topic = factor(topic)) %>%
  ggplot(aes(x = reorder(topic, count, function(x) {-x}), y = pct)) +
  geom_bar(stat = "identity", fill = "lightgray", width = .75) +
  labs(x = "Topic number", y = "Expected topic proportions") + theme_sparse +
  scale_y_continuous(expand = c(0, 0), limits = c(0, .06), breaks = seq(0, .06, by = .01), labels = scales::percent) +
  theme(axis.text.x = element_text(angle = 90, vjust = .75))

plot(lda3)

# Top 10 terms for the topics
(lda_popular <- terms(models40f, k = 15)[, lda_z_counts$topic] %>% data.frame())
```
```{r}
# Save results
write(lda_popular %>% knitr::kable(), file = "results/lda_final_40_top_terms.md")
names(lda_popular) <- names(lda_popular) %>% str_replace(pattern = "Topic.", replacement = "")
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(lda3), width = 7, height = 3, vector.graphic = TRUE)
mydoc <- addFlexTable(mydoc, flextable = FlexTable(lda_popular[, 1:9]))
writeDoc(mydoc, file = "results/fig/3-LDA-topics.docx")
```

# CTM

```{r}
# Model timing
ctm_times <- data.frame(K = rep(seq(20, 120, by = 10), times = 3), heldout = rep(1:3, each = 11),
  time = c(
    ctm_20t4$time, ctm_30t4$time, ctm_40t4$time, ctm_50t4$time, ctm_60t4$time, ctm_70t4$time, ctm_80t4$time,
    ctm_90t4$time, ctm_100t4$time, ctm_110t4$time, ctm_120t4$time,
    ctm_20h2$time, ctm_30h2$time, ctm_40h2$time, ctm_50h2$time, ctm_60h2$time, ctm_70h2$time, ctm_80h2$time,
    ctm_90h2$time, ctm_100h2$time, ctm_110h2$time, ctm_120h2$time,
    ctm_20h3$time, ctm_30h3$time, ctm_40h3$time, ctm_50h3$time, ctm_60h3$time, ctm_70h3$time, ctm_80h3$time,
    ctm_90h3$time, ctm_100h3$time, ctm_110h3$time, ctm_120h3$time
  )/60,
  converge = c(
    ctm_20t4$convergence$its, ctm_30t4$convergence$its, ctm_40t4$convergence$its, ctm_50t4$convergence$its, ctm_60t4$convergence$its, ctm_70t4$convergence$its, ctm_80t4$convergence$its,
    ctm_90t4$convergence$its, ctm_100t4$convergence$its, ctm_110t4$convergence$its, ctm_120t4$convergence$its,
    ctm_20h2$convergence$its, ctm_30h2$convergence$its, ctm_40h2$convergence$its, ctm_50h2$convergence$its, ctm_60h2$convergence$its, ctm_70h2$convergence$its, ctm_80h2$convergence$its,
    ctm_90h2$convergence$its, ctm_100h2$convergence$its, ctm_110h2$convergence$its, ctm_120h2$convergence$its,
    ctm_20h3$convergence$its, ctm_30h3$convergence$its, ctm_40h3$convergence$its, ctm_50h3$convergence$its, ctm_60h3$convergence$its, ctm_70h3$convergence$its, ctm_80h3$convergence$its,
    ctm_90h3$convergence$its, ctm_100h3$convergence$its, ctm_110h3$convergence$its, ctm_120h3$convergence$its
  )
)

write.csv(ctm_times, "results/ctm_times.csv", row.names = FALSE)
```

```{r, rows.print = 11}
# Load ctm data
ctm_times <- read.csv("results/ctm_times.csv")
ctm_heldout_lik <- read.csv("results/ctm_heldout_lik.csv")

# Iterations
ctm_times %>% group_by(K) %>% summarise(mean_converge = mean(converge))
```

## FIG. CTM timing and likelihood

```{r}
# Timing plots
ctm1 <- ctm_times %>% group_by(K) %>% summarise(time = mean(time)) %>%
  ggplot(aes(x = K, y = time)) + geom_smooth(method = "lm", alpha = .5, fullrange = TRUE) + geom_point() +
  theme_sparse + scale_y_continuous(expand = c(0.03, 0)) + scale_x_continuous(limits = c(19, 126)) +
  labs(x = expression(paste(italic("K"))), y = "Time elapsed (minutes)")

plot(ctm1)

# Out-of-sample plots
ctm2 <- ctm_heldout_lik %>% group_by(K, holdout) %>% summarise(mean.lik = mean(lik)) %>%
  left_join(ctm_heldout_lik %>% group_by(K) %>% summarise(gmean.lik = mean(lik)), by = c("K" = "K")) %>%
  ggplot(aes(x = K, y = mean.lik, color = as.factor(holdout))) + geom_point(alpha = .5) +
  scale_x_continuous(breaks = seq(20, 120, by = 10)) +
  geom_line(aes(y = gmean.lik), color = "blue", alpha = .5, size = 1) + theme_sparse +
  labs(color = "Holdout group", x = expression(paste(italic("K"))), y = "Mean per-token holdout log likelihood")

plot(ctm2)
```
```{r}
# Save results
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(ctm1), width = 5, height = 3, vector.graphic = TRUE)
mydoc <- addPlot(mydoc, function() print(ctm2), width = 5, height = 2.5, vector.graphic = TRUE)
writeDoc(mydoc, file = "results/fig/4-CTM-K.docx")
```

# Best CTM model

```{r}
load("results/ctm_final_110.zip")
```

```{r, fig.height = 20}
# How frequently do each of the topics show up?
ctm_t_counts <- ctm_final_110$theta %>% colSums() %>% data.frame(topic = 1:110, n = .)
(ctm_t_counts <- ctm_t_counts %>% mutate(pct = n/sum(n)) %>% arrange(desc(n)) %>% mutate(cum_pct = cumsum(pct)))
```

## FIG. CTM topics

```{r}
# Topic correlation
ctm_corrs <- topicCorr(ctm_final_110, cutoff = .05)
```

```{r, fig.width = 6, fig.height = 6, rows.print = 15}
# Topic probabilities
ctm3 <- ctm_t_counts %>% filter(topic %in% head(ctm_t_counts$topic, 40)) %>% mutate(topic = factor(topic)) %>%
  ggplot(aes(x = reorder(topic, n, function(x) {-x}), y = pct)) +
  geom_bar(stat = "identity", fill = "lightgray", width = .75) +
  labs(x = "Topic number", y = "Expected topic proportions") + theme_sparse +
  scale_y_continuous(expand = c(0, 0), labels = scales::percent, limits = c(0, .03)) +
  theme(axis.text.x = element_text(angle = 90, vjust = .75))

plot(ctm3)

# Topic correlations
ctm_labels <- apply(cbind(1:110, labelTopics(ctm_final_110, n = 2)$frex), MARGIN = 1, FUN = paste, collapse = "\n")
ctm_top_topics <- ctm_t_counts$topic %>% head(40)

set.seed(123)
plot(ctm_corrs, topics = ctm_top_topics, vlabels = ctm_labels[ctm_top_topics],
       vertex.color = "white", vertex.frame.color = "gray", edge.color="gray")

# Top 10 terms for the topics
(ctm_popular <- {labelTopics(ctm_final_110, n = 15)$frex %>% t() %>% data.frame(stringsAsFactors = FALSE)}[, ctm_t_counts$topic])
```

```{r}
# Save results
names(ctm_popular) <- names(ctm_popular) %>% str_replace(pattern = "X", replacement = "Topic.")
write(ctm_popular %>% knitr::kable(), file = "results/ctm_final_110_top_terms.md")
names(ctm_popular) <- names(ctm_popular) %>% str_replace(pattern = "Topic.", replacement = "")
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(ctm3), width = 7, height = 3, vector.graphic = TRUE)
mydoc <- addFlexTable(mydoc, flextable = FlexTable(ctm_popular[, 1:9]))
set.seed(123)
mydoc <- addPlot(mydoc, function() {
  plot(ctm_corrs, topics = ctm_top_topics, vlabels = ctm_labels[ctm_top_topics],
       vertex.color = "white", vertex.frame.color = "gray", edge.color="gray")
1}, width = 8, height = 8, vector.graphic = TRUE)
writeDoc(mydoc, file = "results/fig/5-CTM-topics.docx")
```

# Metadata info

```{r}
# Load metadata
load("data/posts_metadata.zip")
```

```{r}
summary(metadata)
sum(metadata$HW_Tag)
```

```{r}
# Summary stats table
summary_stats <- function(x) {
  data.frame(Minimum = min(x), Q1 = quantile(x, .25), Median = median(x),
  Mean = mean(x), Q3 = quantile(x, .75), Maximum = max(x))
}

sum_stats <- data.frame(
  c("Minimum", "First Quartile", "Median", "Mean", "Third Quartile", "Maximum"),
  summary_stats(metadata$CreationDate %>% floor_date(unit = "days")) %>% t(),
  summary_stats(metadata$LastActivityDate %>% floor_date(unit = "days")) %>% t(),
  summary_stats(metadata$ViewCount) %>% t(),
  summary_stats(metadata$AnswerCount) %>% t(),
  summary_stats(metadata$Score) %>% t(),
  stringsAsFactors = FALSE
)
names(sum_stats) <- c("Statistic", "CreationDate", "LastActivityDate", "ViewCount", "AnswerCount", "Score")
sum_stats
```

## FIG. Metadata stats

```{r}
mydoc <- docx()
mydoc <- addFlexTable(mydoc, flextable = FlexTable(sum_stats))
writeDoc(mydoc, file = "results/fig/6-metadata.docx")
```

# STM

```{r}
# Load data
stm_times <- read.csv("results/stm_times.csv")
stm_heldout_lik <- read.csv("results/stm_heldout_lik.csv")
```

## FIG. STM timing, likelihood

```{r}
# Model runtime
stm1 <- stm_times %>% group_by(K) %>% summarise(time = mean(time)/60) %>%
  ggplot(aes(x = K, y = time)) + geom_smooth(method = "lm", alpha = .5, fullrange = TRUE) + geom_point() +
  theme_sparse + scale_x_continuous(limits = c(53, 123), breaks = seq(60, 130, by = 10)) +
  scale_y_continuous(breaks = seq(0, 7, by = 1), expand = c(0.001, 0), limits = c(0, 8)) +
  labs(x = expression(paste(italic("K"))), y = "Time elapsed (hours)")

plot(stm1)

# Maximum likelihood results

stm2 <- stm_heldout_lik %>% group_by(K, holdout) %>% summarise(mean.lik = mean(lik)) %>%
  left_join(stm_heldout_lik %>% group_by(K) %>% summarise(gmean.lik = mean(lik)), by = c("K" = "K")) %>%
  ggplot(aes(x = K, y = mean.lik, color = as.factor(holdout))) + geom_point(alpha = .5) +
  scale_x_continuous(breaks = seq(60, 130, by = 10), limits = c(53, 123), expand = c(0.03, 0)) +
  ylim(c(-7.36, -7.30)) +
  geom_line(aes(y = gmean.lik), color = "blue", alpha = .5, size = 1) + theme_sparse +
  labs(color = "Holdout group", x = expression(paste(italic("K"))), y = "Mean per-token holdout log likelihood")

plot(stm2)
```
```{r}
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(stm1), width = 5, height = 3, vector.graphic = TRUE)
mydoc <- addPlot(mydoc, function() print(stm2), width = 5, height = 3, vector.graphic = TRUE)
writeDoc(mydoc, file = "results/fig/7-STM-K.docx")
```

# Best STM

```{r}
# Load data
load("results/stm_final_90.zip")
```

```{r}
# Topic frequencies
stm_t_counts <- stm_final_90$theta %>% colSums() %>% data.frame(topic = 1:90, n = .)
(stm_t_counts <- stm_t_counts %>% mutate(pct = n/sum(n)) %>% arrange(desc(n)) %>% mutate(cum_pct = cumsum(pct)))
```

# FIG. STM topics and correlation

```{r}
# Correlations
stm_corrs <- topicCorr(stm_final_90, cutoff = .05)
```

# FIG. STM metadata exploration

```{r, fig.width = 9, fig.height = 6, rows.print = 15}
# Topic probabilities
stm3 <- stm_t_counts %>% filter(topic %in% head(stm_t_counts$topic, 40)) %>% mutate(topic = factor(topic)) %>%
  ggplot(aes(x = reorder(topic, n, function(x) {-x}), y = pct)) +
  geom_bar(stat = "identity", fill = "lightgray", width = .75) +
  labs(x = "Topic number", y = "Expected topic proportions") + theme_sparse +
  scale_y_continuous(expand = c(0, 0), labels = scales::percent, limits = c(0, .032)) +
  theme(axis.text.x = element_text(angle = 90, vjust = .75))

plot(stm3)

# Topic correlations
stm_labels <- apply(cbind(1:90, sageLabels(stm_final_90, n = 2)$marginal$frex), MARGIN = 1, FUN = paste, collapse = "\n")
stm_top_topics <- stm_t_counts$topic %>% head(40)

set.seed(111)
plot(stm_corrs, topics = stm_top_topics, vlabels = stm_labels[stm_top_topics],
       vertex.color = "white", vertex.frame.color = "gray", edge.color="gray")

# Top 10 terms for the topics
(stm_popular <- {sageLabels(stm_final_90, n = 15)$marginal$frex %>% t() %>% data.frame(stringsAsFactors = FALSE)}[, stm_t_counts$topic])
```

```{r}
# Save results
names(stm_popular) <- names(stm_popular) %>% str_replace(pattern = "X", replacement = "Topic.")
write(stm_popular %>% knitr::kable(), file = "results/stm_final_90_top_terms.md")
names(stm_popular) <- names(stm_popular) %>% str_replace(pattern = "Topic.", replacement = "")
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(stm3), width = 7, height = 3, vector.graphic = TRUE)
mydoc <- addFlexTable(mydoc, flextable = FlexTable(stm_popular[, 1:9]))
set.seed(111)
mydoc <- addPlot(mydoc, function() {
  plot(stm_corrs, topics = stm_top_topics, vlabels = stm_labels[stm_top_topics],
       vertex.color = "white", vertex.frame.color = "gray", edge.color="gray")
1}, width = 8, height = 8, vector.graphic = TRUE)
writeDoc(mydoc, file = "results/fig/8-STM-topics.docx")
```

