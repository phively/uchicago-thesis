---
title: "Data creation and parsing"
output: html_notebook
---

# Goals

  * Create reusable syntax splitting code and LaTeX syntax from Body
  * Create a final datafile that can be cleaned normally

# Notes

## HTML parsing

  * Previously, we learned that programming languages in Body really throw off things like word counts
  * Most StackExchange users place code inside \<code\>\<\\code\> sections
  * HTML code can be removed by an XML parser like xml2
  
## TeX parsing

  * StackExchange appears to use the MathJax library; ideally LaTeX extraction would be done using its parser
  * LaTeX code is delimited by \$ (inline) or \$\$ (display)
  * Via trial and error, as of 2017-03-01 MathJax appears to parse *any* text between non-escaped (via backslash) \$ as an inline equation
  
## Suggested process

  1) Extract everything inside \<code\> sections as-is with an XML or HTML parser
  2) Extract everything between non-escaped \$\$ as-is with a regular expression
  3) Extract everything between non-escaped \$ as-is with a regular expression
  4) Strip remaining HTML tags with an XML or HTML parser

# Libraries used

```{r, warning=F, message=F, error=F, comment=F}
library(xml2)
library(dplyr)
library(lubridate)
library(stringr)
library(ggplot2)
```

# Load the raw data

This code is recycled from [01-exploration-and-parsing.Rmd](https://github.com/phively/uchicago-thesis/blob/master/01-exploration-and-parsing.Rmd).

```{r}
# Grab the XML file
xml <- unzip("data/Posts.zip") %>% read_xml() # unz() crashes on me
file.remove("Posts.xml")

# Load post types dictionary
PostTypes <- read.csv("data/PostTypes.csv")

# Function to grab xml fields
getXmlField <- function(part) {xml_children(xml) %>% xml_attr(part, default = "")}

# Data frame to store posts data
posts <- data.frame(stringsAsFactors = FALSE,
  Id = getXmlField("Id") %>% as.numeric(),
  PostTypeId = getXmlField("PostTypeId") %>% as.numeric(),
  AcceptedAnswerId = getXmlField("AcceptedAnswerId") %>% as.numeric(),
  CreationDate = getXmlField("CreationDate") %>% ymd_hms(),
  Score = getXmlField("Score") %>% as.numeric(),
  ViewCount = getXmlField("ViewCount") %>% as.numeric(),
  Body = getXmlField("Body"),
  OwnerUserId = getXmlField("OwnerUserId") %>% as.numeric(),
  LastActivityDate = getXmlField("LastActivityDate") %>% ymd_hms(),
  Title = getXmlField("Title"),
  Tags = getXmlField("Tags"),
  AnswerCount = getXmlField("AnswerCount") %>% as.numeric(),
  CommentCount = getXmlField("CommentCount") %>% as.numeric(),
  FavoriteCount = getXmlField("FavoriteCount") %>% as.numeric(),
  LastEditorUserId = getXmlField("LastEditorUserId") %>% as.numeric(),
  LastEditDate = getXmlField("LastEditDate") %>% ymd_hms(),
  CommunityOwnedDate = getXmlField("CommunityOwnedDate") %>% ymd_hms(),
  ParentId = getXmlField("ParentId") %>% as.numeric(),
  ClosedDate = getXmlField("ClosedDate") %>% ymd_hms()
) %>%
  left_join(PostTypes, by = c("PostTypeId" = "Id")) %>% # Join post type names
  filter(Name %in% c("Question", "Answer")) %>% # Drop posts that are not questions or answers
  mutate(CleanBody = paste(Title, Body, sep = " ")) # Concatenate Title to left of Body
```

# Parse all \<code\>

```{r}
# Add a Code column to store things between code tags
posts <- posts %>% mutate(
  # Collapse all found <code> blocks with a whitespace separator
  Code = Body %>%
    lapply(FUN = read_html) %>% # For each post in Body, read as an html file
    lapply(FUN = xml_find_all, xpath = ".//code") %>% # Find each <code></code> block in html file
    lapply(paste, collapse = " ") %>% # Collapse all found <code> blocks with a whitespace separator
    unlist(),
  # Iterate through each Body removing the identified code
  CleanBody = CleanBody %>%
    gsub(pattern = "<code>.*?</code>", replacement = "", x = .) # .*? enforces a non-greedy match
)
```

# Parse all double \$\$

```{r}
# Add a Display column to store display LaTeX
posts <- posts %>% mutate(
  # Collapse all found $$ ... $$ with a whitespace separator
  DisplayLaTeX = CleanBody %>%
    str_extract_all("\\$\\$.*?\\$\\$") %>% # Find each $$ ... $$ block in Body
    lapply(paste, collapse = " ") %>% # Collapse all found $$ blocks with a whitespace separator
    unlist(),
  # Iterate through each Body removing the identified LaTeX
  CleanBody = CleanBody %>%
    str_replace_all(pattern = "\\$\\$.*?\\$\\$", replacement = "") # .*? enforces a non-greedy match
)
```

# Parse all single \$

Be careful not to include \\\$ as a valid inline open or close.

```{r}
# Add an Inline column to store inline LaTeX
posts <- posts %>% mutate(
  # Collapse all found $ ... $ with a whitespace separator
  # Need to exclude \$ as an open or close
  InlineLaTeX = CleanBody %>%
    str_extract_all("(?<!\\\\)\\$.*?(?<!\\\\)\\$") %>% # (?<!x) is a negative lookbehind for x
    lapply(paste, collapse = " ") %>% # Collapse the $ blocks with a whitespace separator
    unlist(),
  # Iterate through Body and remove the matches
  CleanBody = CleanBody %>%
    str_replace_all(pattern = "(?<!\\\\)\\$.*?(?<!\\\\)\\$", replacement = "")
)
```

# Strip remaining HTML tags

```{r}
posts <- posts %>% mutate(
  CleanBody = CleanBody %>%
    lapply(FUN = read_html) %>% # Read CleanBody as an html file
    lapply(FUN = xml_text) %>% # Extract any text between valid <tags></tags>
    unlist() %>%
    # Remove URLs starting with http and ending with a space
    str_replace_all("http[^[:space:]]*", " ") %>%
    # Remove newlines
    str_replace_all("[\r\n]" , " ")
)
```

# Look at a few examples

```{r}
# Posts identified as interesting
posts %>%
  filter(Id %in% c(35, 73, 78, 85, 90, 101, 118, 227)) %>%
  select(Body, CleanBody, Code, DisplayLaTeX, InlineLaTeX)
```

Everything looks good, so write the file to disk.

# Save datafile to disk

```{r}
posts %>% write.csv(file = "data/posts.csv", row.names = FALSE)
```