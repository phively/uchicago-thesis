---
title: "First LDA"
output: html_notebook
---

# Goals

  * Create a suitable corpus from the dataset
  * See how a simple topic model performs as a baseline for comparing other methods
  
# Corpus cleaning  

  * Perform standard cleaning, e.g. punctuation and number removal, uniform case, etc.
  * Identify a suitable set of stopwords for use throughout

# Libraries used

```{r, warning=F, message=F, error=F, comment=F}
library(dplyr)
library(ggplot2)
library(stringr)
library(tm)
library(slam)
library(topicmodels)
```

# Load the data

This section parses the zipped `posts.csv` created by [03-datafile-construction.Rmd](https://github.com/phively/uchicago-thesis/blob/master/03-datafile-construction.Rmd)

Source code: [read-PostsData-zip.R](https://github.com/phively/uchicago-thesis/blob/master/R/read-PostsData-zip.R)

```{r, message=F}
source("R/read-PostsData-zip.R")
```

# Text cleaning

See R's predefined text patterns.

```{r}
see_pattern <- function(pattern) {
  as.raw(1:(2^8 - 1)) %>% rawToChar() %>% str_replace_all(., paste("[^", pattern, "]"), "") %>% trimws() %>% print()
}

see_pattern("[:punct:]")
```

Note that [:punct:] includes apostrophes which could be problematic for identifying stopwords in later steps. Try a custom string instead.

```{r}
see_pattern("^[:letter:][:space:]'")
```

```{r}
merged <- merged %>% mutate(
  # Convert everything to lower-case
  CleanBody = str_to_lower(CleanBody),
  # Replace everything except letters, spaces, and apostrophes with space
  CleanBody = str_replace_all(CleanBody, "[^[:letter:][:space:]']", " "),
  # Replace multiple whitespaces with a single space
  CleanBody = str_replace_all(CleanBody, "\\s+", " ")
)
```

# Corpus creation

The tm package includes a few standard English stopwords which I'll use to get things started.

```{r}
corpus <- Corpus(VectorSource(merged$CleanBody)) %>% # Read in the merged posts
  tm_map(removeWords, stopwords("english")) %>% # Strip standard stopwords
  tm_map(removePunctuation) # Strip any remaining apostrophes
```

Next, construct the document-term matrix.

```{r}
term_matrix <- DocumentTermMatrix(corpus)
```

Some summary statistics.

```{r}
# Term matrix statistics
print(term_matrix)

# Number of terms appearing at least x times
findFreqTerms(term_matrix, lowfreq = 15) %>% length()
findFreqTerms(term_matrix, lowfreq = 20) %>% length()
findFreqTerms(term_matrix, lowfreq = 25) %>% length()
findFreqTerms(term_matrix, lowfreq = 30) %>% length()
```

Over 12,000 words appear 30 or more times. The most frequently occurring words are as follows.

```{r}
# Word counts with slam::col_sums
wordcounts <- col_sums(term_matrix)
# Print the top 50 words
wordcounts %>% sort(decreasing = TRUE) %>% head(100) %>%
  data.frame(word = names(.), count = ., row.names = NULL, stringsAsFactors = FALSE)
```

# Topic modeling

From the above list, it looks like I could remove some more stopwords -- can, like, get, and so on. Using a list of stopwords defined in [MySQL 5.7](https://dev.mysql.com/doc/refman/5.7/en/fulltext-stopwords.html) to update the corpus:

```{r}
mysql_stopwords <- read.csv("data/MySQL stopwords.txt", header = FALSE, stringsAsFactors = FALSE)[, 1]

# Count of additional stopwords
length(mysql_stopwords)
```
```{r}
corpus <- corpus %>%
  tm_map(removeWords, mysql_stopwords) # Remove custom stopwords
```

Updating the term matrix:

```{r}
term_matrix <- DocumentTermMatrix(corpus)
```
```{r}
# Term matrix statistics
print(term_matrix)

findFreqTerms(term_matrix, lowfreq = 30) %>% length()
```

Prune sparse terms that appear less than 30 times from the term matrix.

```{r}
# Identify which vocabulary terms should be kept
sparse_vocab <- {col_sums(term_matrix) >= 30} %>% unname()
# Identify which columns j of the matrix correspond to these terms
sparse_ind <- {term_matrix$j %in% {1:length(term_matrix$dimnames$Terms)}[sparse_vocab]} %>% unname()
# Map between old and new column numbers
sparse_jmap_old <- 1:length(sparse_vocab)
sparse_jmap_new <- {cumsum(sparse_vocab) * sparse_vocab} %>% unname()

# Create sparse matrix keeping the elements that meet the above criteria
sparse <- term_matrix
sparse$i <- sparse$i[sparse_ind]
sparse$j <- sparse$j[sparse_ind]
sparse$j <- sparse_jmap_new[sparse$j]
sparse$v <- sparse$v[sparse_ind]
sparse$ncol <- sum(sparse_vocab)
sparse$dimnames$Terms <- sparse$dimnames$Terms[sparse_vocab]

# Cleanup
remove(sparse_vocab, sparse_ind, sparse_jmap_old, sparse_jmap_new)
```

Check that the new top vocabulary does not still contain stopwords.

```{r}
# New top vocabulary
col_sums(sparse) %>% sort(decreasing = TRUE) %>% head(100) %>%
  data.frame(word = names(.), count = ., row.names = NULL, stringsAsFactors = FALSE)
```

Looks good. Try fitting a 15-topic model.

```{r}
topic <- LDA(sparse, k = 15, method = "VEM", control = list(seed = 48029, verbose = 1))
save(topic, file = "results/lda15.zip", compress = "xz")
```

The top 20 terms for each topic follow.

```{r rows.print=20}
terms(topic, 20) %>% data.frame(stringsAsFactors = FALSE)
```

Finally, model perplexity scores.

```{r}
perplexity(topic)
```