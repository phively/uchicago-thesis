---
title: "Facts and figures"
output: html_notebook
---

# Libraries

```{r}
library(tidyverse)
library(lubridate)
library(stringr)
library(e1071)
library(ReporteRs)
library(slam)
library(tm)
library(topicmodels)
library(stm)
library(foreach)
```

# Constants

```{r}
theme_sparse <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
  panel.background = element_blank(), axis.line = element_line(colour = "black"), legend.key = element_blank())
```

# Data exploration

```{r}
# Load posts
posts <- unz("data/PostsData.zip", filename = "posts.csv") %>%
  read.csv(header = TRUE, sep = ",", stringsAsFactors = FALSE)

# Fix data fields
posts <- posts %>% mutate(
  CreationDate = ymd_hms(CreationDate),
  Name = factor(Name)
)
```

```{r}
# Post stats
summary(posts$Name)
min(posts$CreationDate)
max(posts$CreationDate)
```

```{r}
# Load data file
load(file = "data/sparse_matrix_no_tex.zip")
```

```{r}
# Data file stats
sparse$dimnames$Docs %>% length()
sparse$dimnames$Terms %>% length()
print("Min word length")
col_sums(sparse) %>% min()
```

```{r, cache = TRUE}
# Word count comparison
cb_dtm <- DocumentTermMatrix(Corpus(VectorSource(posts$CleanBody)))
```
```{r}
# Full document word count
cb_dtm$dimnames$Terms %>% length()
col_sums(cb_dtm) %>% sum()
# Trimmed document word count
sparse$dimnames$Terms %>% length()
col_sums(sparse) %>% sum()
```

## FIG. Word-document distribution

```{r, fig.width = 5, fig.height = 2.5}
# Data
wc <- row_sums(sparse) %>% as.numeric()
# Stats
log10(wc) %>% mean()
log10(wc) %>% sd()
log10(wc) %>% skewness()
log10(wc) %>% kurtosis(type = 2) + 3

# Plot
myplot <- wc %>% data.frame(wordcount = .) %>%
  ggplot(aes(x = wordcount)) +
  geom_histogram(binwidth = .06, color = "lightgray", fill = "lightgray") +
  stat_function(
    fun = function(x) {(dnorm(log10(x), mean(log10(wc)), sd(log10(wc)))) * length(wc) * .06},
    color = "blue", size = 1, alpha = .3
  ) +
  scale_x_log10(breaks = c(0, 1, 10, 100, 1000, 10000)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Word count", y = "Document count") +
  theme_sparse

plot(myplot)

mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(myplot), width = 5, height = 3, vector.graphic = TRUE)
writeDoc(mydoc, file = "results/fig/1-word-doc-dist.docx")
```

# LDA

```{r}
# Load precomputed model perplexities
lda_results <- read.csv("results/lda_k_perp.csv")
```

```{r}
# Average convergence
lda_results %>% group_by(model) %>% summarise(mean(time))
```

## FIG. LDA runtime and perplexity

```{r}
# LDA runtime
lda1 <- lda_results %>% filter(model >= 10 & model %% 5 == 0) %>% group_by(model) %>% summarise(time = mean(time)) %>%
  ggplot(aes(x = model, y = time)) + geom_smooth(method = "lm", alpha = .5, fullrange = TRUE) +
  geom_point() + xlim(c(9, 51)) + scale_y_continuous(breaks = seq(0, 12, by = 2), expand = c(.005, 0)) +
  labs(x = expression(paste(italic("K"))), y = "Time elapsed (hours)") + theme_sparse
plot(lda1)

# LDA perplexity
lda2 <- lda_results %>% filter(model >= 10) %>% group_by(model) %>% mutate(in_avg = mean(in_sample), out_avg = mean(out_sample)) %>%
  ggplot(aes(x = model)) +
  geom_point(aes(y = in_sample, color = "In-sample"), alpha = .5) + geom_line(aes(y = in_avg, color = "In-sample"), size = 1, alpha = .5) +
  geom_point(aes(y = out_sample, color = "Out-sample"), alpha = .5) + geom_line(aes(y = out_avg, color = "Out-sample"), size = 1, alpha = .5) +
  labs(x = expression(paste(italic("K"))), y = "Perplexity", color = "") + theme_sparse 
plot(lda2)  
```

```{r}
# Write plots
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(lda1), width = 5, height = 3, vector.graphic = TRUE)
mydoc <- addPlot(mydoc, function() print(lda2), width = 5, height = 3, vector.graphic = TRUE)
writeDoc(mydoc, file = "results/fig/2-LDA.docx")
```

# Best LDA model

```{r}
# Load model
load("results/lda_final_40.zip")
```

```{r}
# How frequently do each of the topics show up?
lda_z_counts <- models40f@wordassignments$v %>% factor() %>% summary() %>% data.frame(topic = 1:40, count = .)
(lda_z_counts <- lda_z_counts %>%
    mutate(pct = count / sum(count)) %>% arrange(desc(count)) %>% mutate(cumpct = cumsum(pct))
)
```

```{r}
# Most likely documents
lda_likely <- which(models40f@loglikelihood >= tail(sort(models40f@loglikelihood))[1])
{posts %>% select(Id, Title, CleanBody)}[lda_likely, ]
```

## FIG. topic frequency

```{r}
lda3 <- lda_z_counts %>% mutate(topic = factor(topic)) %>%
  ggplot(aes(x = reorder(topic, count, function(x) {-x}), y = pct)) +
  geom_bar(stat = "identity", fill = "lightgray", position = "dodge") +
  labs(x = "Topic number", y = "Topic proportions") + theme_sparse +
  scale_y_continuous(expand = c(0, 0), limits = c(0, .11), breaks = seq(0, .12, by = .02), labels = scales::percent)

plot(lda3)

# Top 10 terms for the topics
(lda_popular <- terms(models40f, k = 15)[, lda_z_counts$topic] %>% data.frame())

# Save results
write(lda_popular %>% knitr::kable(), file = "results/lda_final_40_top_terms.md")
names(lda_popular) <- names(lda_popular) %>% str_replace(pattern = "Topic.", replacement = "")
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(lda3), width = 6, height = 3, vector.graphic = TRUE)
mydoc <- addFlexTable(mydoc, flextable = FlexTable(lda_popular[, 1:9]))
writeDoc(mydoc, file = "results/fig/3-LDA-topics.docx")
```

# CTM

```{r}
# Model timing
ctm_times <- data.frame(K = rep(seq(20, 120, by = 10), times = 3), heldout = rep(1:3, each = 11),
  time = c(
    ctm_20t4$time, ctm_30t4$time, ctm_40t4$time, ctm_50t4$time, ctm_60t4$time, ctm_70t4$time, ctm_80t4$time,
    ctm_90t4$time, ctm_100t4$time, ctm_110t4$time, ctm_120t4$time,
    ctm_20h2$time, ctm_30h2$time, ctm_40h2$time, ctm_50h2$time, ctm_60h2$time, ctm_70h2$time, ctm_80h2$time,
    ctm_90h2$time, ctm_100h2$time, ctm_110h2$time, ctm_120h2$time,
    ctm_20h3$time, ctm_30h3$time, ctm_40h3$time, ctm_50h3$time, ctm_60h3$time, ctm_70h3$time, ctm_80h3$time,
    ctm_90h3$time, ctm_100h3$time, ctm_110h3$time, ctm_120h3$time
  )/60,
  converge = c(
    ctm_20t4$convergence$its, ctm_30t4$convergence$its, ctm_40t4$convergence$its, ctm_50t4$convergence$its, ctm_60t4$convergence$its, ctm_70t4$convergence$its, ctm_80t4$convergence$its,
    ctm_90t4$convergence$its, ctm_100t4$convergence$its, ctm_110t4$convergence$its, ctm_120t4$convergence$its,
    ctm_20h2$convergence$its, ctm_30h2$convergence$its, ctm_40h2$convergence$its, ctm_50h2$convergence$its, ctm_60h2$convergence$its, ctm_70h2$convergence$its, ctm_80h2$convergence$its,
    ctm_90h2$convergence$its, ctm_100h2$convergence$its, ctm_110h2$convergence$its, ctm_120h2$convergence$its,
    ctm_20h3$convergence$its, ctm_30h3$convergence$its, ctm_40h3$convergence$its, ctm_50h3$convergence$its, ctm_60h3$convergence$its, ctm_70h3$convergence$its, ctm_80h3$convergence$its,
    ctm_90h3$convergence$its, ctm_100h3$convergence$its, ctm_110h3$convergence$its, ctm_120h3$convergence$its
  )
)

write.csv(ctm_times, "results/ctm_times.csv", row.names = FALSE)
```

```{r}
# Load ctm data
ctm_times <- read.csv("results/ctm_times.csv")
ctm_heldout_lik <- read.csv("results/ctm_heldout_lik.csv")

# Iterations
ctm_times %>% group_by(K) %>% summarise(mean_converge = mean(converge))
```

## FIG CTM timing and likelihood

```{r}
# Timing plots
ctm1 <- ctm_times %>% group_by(K) %>% summarise(time = mean(time)) %>%
  ggplot(aes(x = K, y = time)) + geom_smooth(method = "lm", alpha = .5, fullrange = TRUE) + geom_point() +
  theme_sparse + scale_y_continuous(expand = c(0.03, 0)) + scale_x_continuous(limits = c(19, 126)) +
  labs(x = expression(paste(italic("K"))), y = "Time elapsed (minutes)")

plot(ctm1)

# Out-of-sample plots
ctm2 <- ctm_heldout_lik %>% group_by(K, holdout) %>% summarise(mean.lik = mean(lik)) %>%
  left_join(ctm_heldout_lik %>% group_by(K) %>% summarise(gmean.lik = mean(lik)), by = c("K" = "K")) %>%
  ggplot(aes(x = K, y = mean.lik, color = as.factor(holdout))) + geom_point(alpha = .5) +
  scale_x_continuous(breaks = seq(20, 120, by = 10)) +
  geom_line(aes(y = gmean.lik), color = "blue", alpha = .5, size = 1) + theme_sparse +
  labs(color = "Holdout group", x = expression(paste(italic("K"))), y = "Mean per-token holdout log likelihood")

plot(ctm2)

# Save results
mydoc <- docx()
mydoc <- addPlot(mydoc, function() print(ctm1), width = 5, height = 3, vector.graphic = TRUE)
mydoc <- addPlot(mydoc, function() print(ctm2), width = 5, height = 2.5, vector.graphic = TRUE)
writeDoc(mydoc, file = "results/fig/4-CTM-K.docx")
```



